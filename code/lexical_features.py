"""
Features include
Relevance 
Distinctiveness 
Style
Log odds features 
Narrative
"""
import json
import numpy
import sys
import re
from collections import defaultdict, Counter
import numpy as np
import time
from nltk.corpus import stopwords
import math
from nltk import ngrams
from nltk.tokenize import word_tokenize, sent_tokenize
import string
from empath import Empath

DATA = "RANK"
if DATA == "GILDS":
    INPUT = "../data/gilded_classifier_comments"
    POST_INPUT = "../data/gilded_classifier_posts"
    PARENT_INPUT = "../data/gilded_classifier_parents"
    OUTPUT = "../logs/gild_lexical_vectors/"
elif DATA == "RANK":
    INPUT = "../data/rank_classifier_comments"
    POST_INPUT = "../data/rank_classifier_posts"
    PARENT_INPUT = "../data/rank_classifier_parents"
    OUTPUT = "../logs/rank_lexical_vectors/"
BROWN_COUNTS = "../logs/brown_counts.json"
SUB_PROBS = "../logs/subreddit_word_probs.json"

def get_relevance(split_text, parent_text, post_text):
    '''
    - a comment's relevance to the parent comment
    - its relevance to the post and post title.
    using jaccard, removing stopwords
    '''
    text = set(split_text)
    sw = set(stopwords.words('english'))
    
    post_text = re.sub(r'[^\w\d\'\s]','',post_text)
    post = set(post_text.lower().encode('utf-8').split())
    post = post - sw
    denom = float(len(post | text))
    if denom == 0.0: 
        post_relevance = 0
    else:
        post_relevance = len(post & text)/denom
    if parent_text: 
        parent_text = re.sub(r'[^\w\d\'\s]','', parent_text)
        parent = set(parent_text.lower().encode('utf-8').split())
        parent = parent - sw
        denom = float(len(parent | text))
        if denom == 0.0:
            parent_relevance = 0
        else:
            parent_relevance = len(parent & text)/denom
    else: 
        parent_relevance = post_relevance
    return [post_relevance, parent_relevance]

def get_brown(orig_text, brown):
    '''
    unigram and bigram likelihood using a 
    "common language" corpus, such as the Brown corpus.
    Language model with laplace smoothing: 
    bigrams: p(w_i | w_(i-1)) = (count(w_i, w_(i-1)) + 1)/(count(w_(i-1)) + V)
    unigrams: p(w_i) = (count(w_i) + 1)/(total counts of all words + V)
    add up log(p) to avoid overflow
    
    Same steps as brown corpus preprocessing in preprocess_feature_data.py
    
    also unigram/bigram/trigram features generated by log_odds
    '''
    text_tok = word_tokenize(orig_text)
    split_text = [re.sub(r'[^\w\d\'\s]','',w).lower() for w \
             in text_tok if w not in string.punctuation]
    sentences = [word_tokenize(t) for t in sent_tokenize(orig_text)]
    bigrams = []
    for s in sentences: 
        s = [re.sub(r'[^\w\d\'\s]','',w).lower() for w \
             in s if w not in string.punctuation]
        s = [w for w in s if w != '']
        bigrams.extend([' '.join(g) for g in ngrams(s, 2)])
        
    total_words = sum(brown['words'].values())
    V = len(brown['words'].keys())
    unigram_prob = 0
    for u in split_text:
        if u in brown['words']:
            unigram_prob += math.log(brown['words'][u] + 1) - math.log(total_words + V)
        else:
            unigram_prob += math.log(1) - math.log(total_words + V)
    V = len(brown['bigrams'].keys())
    bigram_prob = 0
    for b in bigrams: 
        one = b[0]
        if b in brown['bigrams'] and one in brown['words']:
            bigram_prob += math.log(brown['bigrams'][b] + 1) - math.log(brown['words'][one] + V)
        elif one in brown['words']: 
            bigram_prob += math.log(1) - math.log(brown['words'][one] + V)
        else: 
            bigram_prob += math.log(1) - math.log(V)      
    return [unigram_prob, bigram_prob]

def get_style(split_text, word_probs):
    '''
    - posterior probability of a comment appearing in its subreddit
    p(comment containing word) = count(# of comments that word appears in)/
        total number of comments
    add up log(p) to avoid overflow
    Treat comments as sets of words
    '''
    text = set(split_text)
    prob = 0
    for word in text: 
        # should not be zero since processed these counts on the data
        prob += math.log(word_probs[word])
    return [prob]

def get_empath(text):
    lexicon = Empath()
    categories = sorted(lexicon.cats.keys())
    res = lexicon.analyze(text, normalize=True)
    features = []
    for i in range(len(categories)):
        if res: 
            features.append(res[categories[i]])
        else:
            features.append(0)
    return features

def get_features(orig_text, post_id, comment_id, subreddit, edited, \
                 parent_id, parent_dict, post_dict, brown, word_sub_probs):
    '''
    @input
        - 
    @return
        - 
    '''
    if edited: 
        # remove edits, since they may thank the gilder after the fact
        text_lines = orig_text.split('\n')
        text_lines = [line for line in text_lines if 'Edit' not in line and \
                              'ETA' not in line and 'edit' not in line \
                              and 'eta' not in line and 'gold' not in line and \
                                  'Gold' not in line and 'gild' not in line]
        orig_text = '\n'.join(text_lines)
    text = re.sub(r'[^\w\d\'\s]','',orig_text)
    text = text.lower().encode('utf-8')
    p_dict = post_dict[subreddit][post_id]
    post_text = p_dict['title'] + ' ' + p_dict['text']
    parent_id = parent_id.split('_')[-1]
    if parent_id == post_id:  
        parent_text = None
    elif parent_id == 'cr2vv8s':
        # cr2vv8s is a strange edge case parent that somehow 
        # doesn't exist in the raw comments data file.
        parent_text = ''
    else: 
        parent_text = parent_dict[subreddit][post_id][parent_id]
    split_text = text.split()
    rel_feat = get_relevance(split_text, parent_text, post_text)
    
    brown_feat = get_brown(orig_text, brown)
    empath_feat = get_empath(text)
    style_feat = get_style(split_text, word_sub_probs[subreddit])
    all_feats = np.array(rel_feat + style_feat + empath_feat + brown_feat)
    np.save(OUTPUT + subreddit + '_' + post_id + '_' + comment_id + '.npy', all_feats)

def main():
    with open(POST_INPUT, 'r') as post_inputfile:
        # posts of comments
        post_dict = json.load(post_inputfile)
    with open(PARENT_INPUT, 'r') as parent_inputfile:
        # parents of comments
        parent_dict = json.load(parent_inputfile)
    with open(INPUT, 'r') as input_file:
        # all comments in dataset
        all_comments = json.load(input_file)
    with open(BROWN_COUNTS, 'r') as brown_file: 
        # bigram and unigram counts of words in brown corpus
        brown = json.load(brown_file)
    with open(SUB_PROBS, 'r') as sub_probs_file: 
        # word_sub_probs[subreddit][word] = probability of word 
        # in comment in that subreddit
        word_sub_probs = json.load(sub_probs_file)
    i = 0
    for subreddit in all_comments:
        posts = all_comments[subreddit]
        for post_id in posts:
            for comment_id in posts[post_id]:
                comment = posts[post_id][comment_id]
                if i % 2000 == 0: print time.time()
                i += 1
                get_features(comment['body'], post_id, comment_id, \
                             subreddit, comment['edited'], \
                             comment['parent_id'], parent_dict, post_dict, \
                             brown, word_sub_probs)

if __name__ == "__main__":
    main()
